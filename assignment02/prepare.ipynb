{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitted by: Sampriti Mahapatra, MDS202433"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Classification - Data Preparation\n",
    "\n",
    "This notebook loads, preprocesses, splits, and saves the SMS spam dataset for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Project directory\n",
    "PROJECT_DIR = '/Users/sampriti/Downloads/cmi/AML_2'\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Load SMS spam data from tab-separated file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    # Read tab-separated file with proper encoding\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None, names=['label', 'message'], encoding='utf-8')\n",
    "    \n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(df['label'].value_counts())\n",
    "    print(f\"\\nClass percentages:\")\n",
    "    print(df['label'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"\\nMissing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nDuplicate rows: {duplicates}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 5572\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class percentages:\n",
      "label\n",
      "ham     86.593683\n",
      "spam    13.406317\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Missing values:\n",
      "label      0\n",
      "message    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows: 403\n",
      "\n",
      "Raw data saved to /Users/sampriti/Downloads/cmi/AML_2/raw_data.csv\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the raw SMS data\n",
    "df = load_data(os.path.join(PROJECT_DIR, 'sms+spam+collection/SMSSpamCollection'))\n",
    "\n",
    "# Save the raw loaded data as raw_data.csv (before preprocessing)\n",
    "raw_data_path = os.path.join(PROJECT_DIR, 'raw_data.csv')\n",
    "df.to_csv(raw_data_path, index=False)\n",
    "print(f\"\\nRaw data saved to {raw_data_path}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample HAM messages:\n",
      "1. If i not meeting ü all rite then i'll go home lor. If ü dun feel like comin it's ok.\n",
      "\n",
      "2. I.ll always be there, even if its just in spirit. I.ll get a bb soon. Just trying to be sure i need it.\n",
      "\n",
      "3. Sorry that took so long, omw now\n",
      "\n",
      "\n",
      "Sample SPAM messages:\n",
      "1. Summers finally here! Fancy a chat or flirt with sexy singles in yr area? To get MATCHED up just reply SUMMER now. Free 2 Join. OptOut txt STOP Help08714742804\n",
      "\n",
      "2. This is the 2nd time we have tried 2 contact u. U have won the 750 Pound prize. 2 claim is easy, call 08718726970 NOW! Only 10p per min. BT-national-rate \n",
      "\n",
      "3. Get ur 1st RINGTONE FREE NOW! Reply to this msg with TONE. Gr8 TOP 20 tones to your phone every week just £1.50 per wk 2 opt out send STOP 08452810071 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample messages from each class\n",
    "print(\"Sample HAM messages:\")\n",
    "for i, msg in enumerate(df[df['label'] == 'ham']['message'].sample(3, random_state=RANDOM_STATE).values, 1):\n",
    "    print(f\"{i}. {msg}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nSample SPAM messages:\")\n",
    "for i, msg in enumerate(df[df['label'] == 'spam']['message'].sample(3, random_state=RANDOM_STATE).values, 1):\n",
    "    print(f\"{i}. {msg}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace URLs with token\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', 'URL', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Replace phone numbers with token (various formats)\n",
    "    text = re.sub(r'\\b\\d{5,}\\b', 'PHONE', text)  # 5+ consecutive digits\n",
    "    text = re.sub(r'\\+?\\d[\\d\\s\\-\\(\\)]{7,}\\d', 'PHONE', text)  # Phone formats\n",
    "    \n",
    "    # Replace currency symbols with token\n",
    "    text = re.sub(r'[£$€¥₹]', 'CURRENCY', text)\n",
    "    \n",
    "    # Replace numbers with token (but keep single digits for now)\n",
    "    text = re.sub(r'\\b\\d{2,}\\b', 'NUMBER', text)\n",
    "    \n",
    "    # Remove special characters but keep spaces\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing all messages...\n",
      "Empty messages after cleaning: 3\n",
      "\n",
      "Cleaning Results (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>cleaned_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3245</th>\n",
       "      <td>Squeeeeeze!! This is christmas hug.. If u lik ...</td>\n",
       "      <td>squeeeeeze this is christmas hug if u lik my f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>And also I've sorta blown him off a couple tim...</td>\n",
       "      <td>and also i ve sorta blown him off a couple tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>Mmm thats better now i got a roast down me! i...</td>\n",
       "      <td>mmm thats better now i got a roast down me i d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>Mm have some kanji dont eat anything heavy ok</td>\n",
       "      <td>mm have some kanji dont eat anything heavy ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>So there's a ring that comes with the guys cos...</td>\n",
       "      <td>so there s a ring that comes with the guys cos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                message  \\\n",
       "3245  Squeeeeeze!! This is christmas hug.. If u lik ...   \n",
       "944   And also I've sorta blown him off a couple tim...   \n",
       "1044  Mmm thats better now i got a roast down me! i...   \n",
       "2484      Mm have some kanji dont eat anything heavy ok   \n",
       "812   So there's a ring that comes with the guys cos...   \n",
       "\n",
       "                                        cleaned_message  \n",
       "3245  squeeeeeze this is christmas hug if u lik my f...  \n",
       "944   and also i ve sorta blown him off a couple tim...  \n",
       "1044  mmm thats better now i got a roast down me i d...  \n",
       "2484      mm have some kanji dont eat anything heavy ok  \n",
       "812   so there s a ring that comes with the guys cos...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply preprocessing to all messages\n",
    "print(\"Preprocessing all messages...\")\n",
    "df['cleaned_message'] = df['message'].apply(preprocess_text)\n",
    "\n",
    "# Verify no empty messages after cleaning\n",
    "empty_count = (df['cleaned_message'].str.len() == 0).sum()\n",
    "print(f\"Empty messages after cleaning: {empty_count}\")\n",
    "\n",
    "# Display examples\n",
    "print(\"\\nCleaning Results (sample):\")\n",
    "df[['message', 'cleaned_message']].sample(5, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train/test/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, train_size=0.70, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    # Verify proportions sum to 1\n",
    "    assert abs(train_size + val_size + test_size - 1.0) < 0.001, \"Split proportions must sum to 1\"\n",
    "    \n",
    "    # First split: separate test set\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=df['label']\n",
    "    )\n",
    "    \n",
    "    # Second split: separate train and validation\n",
    "    val_proportion = val_size / (train_size + val_size)\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size=val_proportion,\n",
    "        random_state=random_state,\n",
    "        stratify=train_val_df['label']\n",
    "    )\n",
    "    \n",
    "    # Display split statistics\n",
    "    print(\"Data Split Summary:\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"\\nTraining set:   {len(train_df)} samples ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"Validation set: {len(val_df)} samples ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"Test set:       {len(test_df)} samples ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Verify stratification\n",
    "    print(\"\\nClass distribution in each split:\")\n",
    "    \n",
    "    for split_name, split_df in [('Training', train_df), ('Validation', val_df), ('Test', test_df)]:\n",
    "        spam_count = (split_df['label'] == 'spam').sum()\n",
    "        ham_count = (split_df['label'] == 'ham').sum()\n",
    "        spam_pct = spam_count / len(split_df) * 100\n",
    "        print(f\"{split_name:12s}: Ham={ham_count:4d} ({100-spam_pct:5.2f}%), Spam={spam_count:3d} ({spam_pct:5.2f}%)\")\n",
    "    \n",
    "    # Verify no overlap\n",
    "    train_indices = set(train_df.index)\n",
    "    val_indices = set(val_df.index)\n",
    "    test_indices = set(test_df.index)\n",
    "    \n",
    "    assert len(train_indices & val_indices) == 0, \"Train and validation sets overlap\"\n",
    "    assert len(train_indices & test_indices) == 0, \"Train and test sets overlap\"\n",
    "    assert len(val_indices & test_indices) == 0, \"Validation and test sets overlap\"\n",
    "    print(\"No data leakage detected between splits\")\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split Summary:\n",
      "Total samples: 5572\n",
      "\n",
      "Training set:   3900 samples (70.0%)\n",
      "Validation set: 836 samples (15.0%)\n",
      "Test set:       836 samples (15.0%)\n",
      "\n",
      "Class distribution in each split:\n",
      "Training    : Ham=3377 (86.59%), Spam=523 (13.41%)\n",
      "Validation  : Ham= 724 (86.60%), Spam=112 (13.40%)\n",
      "Test        : Ham= 724 (86.60%), Spam=112 (13.40%)\n",
      "No data leakage detected between splits\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "train_df, val_df, test_df = split_data(\n",
    "    df,\n",
    "    train_size=0.70,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering out empty cleaned messages...\n",
      "Before filtering:\n",
      "  Training: 3900 samples\n",
      "  Validation: 836 samples\n",
      "  Test: 836 samples\n",
      "\n",
      "Empty messages to remove:\n",
      "  Training: 2\n",
      "  Validation: 0\n",
      "  Test: 1\n",
      "\n",
      "After filtering:\n",
      "  Training: 3898 samples\n",
      "  Validation: 836 samples\n",
      "  Test: 835 samples\n",
      "  Total: 5569 samples\n",
      "\n",
      "Empty messages removed successfully\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows with empty cleaned_message\n",
    "print(\"\\nFiltering out empty cleaned messages...\")\n",
    "print(f\"Before filtering:\")\n",
    "print(f\"  Training: {len(train_df)} samples\")\n",
    "print(f\"  Validation: {len(val_df)} samples\")\n",
    "print(f\"  Test: {len(test_df)} samples\")\n",
    "\n",
    "# Count empty messages in each split\n",
    "train_empty = (train_df['cleaned_message'].str.len() == 0).sum()\n",
    "val_empty = (val_df['cleaned_message'].str.len() == 0).sum()\n",
    "test_empty = (test_df['cleaned_message'].str.len() == 0).sum()\n",
    "print(f\"\\nEmpty messages to remove:\")\n",
    "print(f\"  Training: {train_empty}\")\n",
    "print(f\"  Validation: {val_empty}\")\n",
    "print(f\"  Test: {test_empty}\")\n",
    "\n",
    "# Filter out empty messages\n",
    "train_df = train_df[train_df['cleaned_message'].str.len() > 0].copy()\n",
    "val_df = val_df[val_df['cleaned_message'].str.len() > 0].copy()\n",
    "test_df = test_df[test_df['cleaned_message'].str.len() > 0].copy()\n",
    "\n",
    "print(f\"\\nAfter filtering:\")\n",
    "print(f\"  Training: {len(train_df)} samples\")\n",
    "print(f\"  Validation: {len(val_df)} samples\")\n",
    "print(f\"  Test: {len(test_df)} samples\")\n",
    "print(f\"  Total: {len(train_df) + len(val_df) + len(test_df)} samples\")\n",
    "print(\"\\nEmpty messages removed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save splits as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_splits(train_df, val_df, test_df, output_dir='.'):\n",
    "    import os\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Define file paths\n",
    "    train_path = os.path.join(output_dir, 'train.csv')\n",
    "    val_path = os.path.join(output_dir, 'validation.csv')\n",
    "    test_path = os.path.join(output_dir, 'test.csv')\n",
    "    \n",
    "    # Select columns to save (label, original message, cleaned message)\n",
    "    columns_to_save = ['label', 'message', 'cleaned_message']\n",
    "    \n",
    "    # Save to CSV\n",
    "    train_df[columns_to_save].to_csv(train_path, index=False)\n",
    "    val_df[columns_to_save].to_csv(val_path, index=False)\n",
    "    test_df[columns_to_save].to_csv(test_path, index=False)\n",
    "    \n",
    "    print(\"Files saved successfully:\")\n",
    "    print(f\"Training set:   {train_path}\")\n",
    "    print(f\"Validation set: {val_path}\")\n",
    "    print(f\"Test set:       {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved successfully:\n",
      "Training set:   /Users/sampriti/Downloads/cmi/AML_2/train.csv\n",
      "Validation set: /Users/sampriti/Downloads/cmi/AML_2/validation.csv\n",
      "Test set:       /Users/sampriti/Downloads/cmi/AML_2/test.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the splits\n",
    "output_directory = PROJECT_DIR  # Fixed: was incorrectly pointing to 'AML 1'\n",
    "save_splits(train_df, val_df, test_df, output_dir=output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Git and DVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitialized existing Git repository in /Users/sampriti/Downloads/cmi/AML_2/.git/\n",
      "\n",
      ".gitignore created\n",
      "\u001b[31mERROR\u001b[39m: failed to initiate DVC - '.dvc' exists. Use `-f` to force.\n",
      "\u001b[0m\n",
      "DVC initialized successfully\n"
     ]
    }
   ],
   "source": [
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "# Initialize git repository\n",
    "!git init\n",
    "!git config user.name \"Sampriti Mahapatra\"\n",
    "!git config user.email \"sampriti@example.com\"\n",
    "\n",
    "# Create .gitignore\n",
    "gitignore_content = \"\"\"# Python\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*.egg-info/\n",
    ".venv/\n",
    "venv/\n",
    "\n",
    "# Jupyter\n",
    ".ipynb_checkpoints/\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "\n",
    "# Data files tracked by DVC\n",
    "/raw_data.csv\n",
    "/train.csv\n",
    "/validation.csv\n",
    "/test.csv\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(PROJECT_DIR, '.gitignore'), 'w') as f:\n",
    "    f.write(gitignore_content.strip())\n",
    "print(\"\\n.gitignore created\")\n",
    "\n",
    "# Initialize DVC\n",
    "!dvc init\n",
    "print(\"\\nDVC initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mkeys\u001b[m\n",
      "\t\u001b[31mmlruns/\u001b[m\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n",
      "\u001b[33m6661081\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD\u001b[m\u001b[33m -> \u001b[m\u001b[1;32mmain\u001b[m\u001b[33m)\u001b[m Add train.ipynb with 3 benchmark models tracked by MLflow\n",
      "\u001b[33m564de3c\u001b[m Add Google Drive as DVC remote storage\n",
      "\u001b[33meaf8eeb\u001b[m configure google drive as dvc remote storage\n",
      "\u001b[33mbff705e\u001b[m configure google drive as dvc remote storage\n",
      "\u001b[33mc4f01f1\u001b[m Version 2: data splits with RANDOM_STATE=123\n",
      "\u001b[33md3baad4\u001b[m Version 1: data splits with RANDOM_STATE=42\n",
      "\u001b[33m4a140a4\u001b[m Initial commit: project setup with DVC initialization\n",
      "\u001b[33mc88235d\u001b[m Add train.ipynb with 3 benchmark models tracked by MLflow\n",
      "\u001b[33m016ac01\u001b[m Add Google Drive as DVC remote storage\n",
      "\u001b[33m70d0695\u001b[m\u001b[33m (\u001b[m\u001b[1;33mtag: \u001b[m\u001b[1;33mv2\u001b[m\u001b[33m)\u001b[m Version 2: data splits with RANDOM_STATE=123\n",
      "\u001b[33md9bca26\u001b[m\u001b[33m (\u001b[m\u001b[1;33mtag: \u001b[m\u001b[1;33mv1\u001b[m\u001b[33m)\u001b[m Version 1: data splits with RANDOM_STATE=42\n",
      "\u001b[33m2b3cda5\u001b[m Initial commit: project setup with DVC initialization\n"
     ]
    }
   ],
   "source": [
    "# Initial commit with project setup\n",
    "!git add .gitignore .dvc/ .dvcignore prepare.ipynb plan.md requirements.txt sms+spam+collection/\n",
    "!git commit -m \"Initial commit: project setup with DVC initialization\"\n",
    "!git log --oneline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Data with DVC - Version 1 (seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[?25l\u001b[32m⠋\u001b[0m Checking graph\n",
      "  0% Adding...|                       | raw_data.csv |0/4 [00:00<?,     ?file/s]\n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in raw_data.csv |0.00 [00:00,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/Users/sampriti/Downloads/cmi/AML_2/.dvc/cache/files/md5\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /Users/sampriti/Downloads0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "  0% Adding...|                          | train.csv |0/4 [00:00<?,     ?file/s]\u001b[A\n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in train.csv    |0.00 [00:00,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/Users/sampriti/Downloads/cmi/AML_2/.dvc/cache/files/md5\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /Users/sampriti/Downloads0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "  0% Adding...|                     | validation.csv |0/4 [00:00<?,     ?file/s]\u001b[A\n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in validation.csv |0.00 [00:00,     ?file/\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/Users/sampriti/Downloads/cmi/AML_2/.dvc/cache/files/md5\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /Users/sampriti/Downloads0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "  0% Adding...|                           | test.csv |0/4 [00:00<?,     ?file/s]\u001b[A\n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in test.csv     |0.00 [00:00,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/Users/sampriti/Downloads/cmi/AML_2/.dvc/cache/files/md5\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /Users/sampriti/Downloads0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|███████████████████████████████████████|4/4 [00:00, 194.86file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add train.csv.dvc raw_data.csv.dvc validation.csv.dvc test.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[0m\n",
      "DVC tracking files created:\n",
      "-rw-r--r--@ 1 sampriti  staff  94 Feb 16 16:13 raw_data.csv.dvc\n",
      "-rw-r--r--@ 1 sampriti  staff  90 Feb 16 16:13 test.csv.dvc\n",
      "-rw-r--r--@ 1 sampriti  staff  91 Feb 16 16:13 train.csv.dvc\n",
      "-rw-r--r--@ 1 sampriti  staff  96 Feb 16 16:13 validation.csv.dvc\n"
     ]
    }
   ],
   "source": [
    "# Track data files with DVC (Version 1 - seed=42)\n",
    "os.chdir(PROJECT_DIR)\n",
    "!dvc add raw_data.csv train.csv validation.csv test.csv\n",
    "\n",
    "# Show the .dvc pointer files created\n",
    "print(\"\\nDVC tracking files created:\")\n",
    "!ls -la *.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 0edda6c] Version 1: data splits with RANDOM_STATE=42\n",
      " 3 files changed, 6 insertions(+), 6 deletions(-)\n",
      "fatal: tag 'v1' already exists\n",
      "\n",
      "Version 1 committed and tagged as 'v1'\n",
      "\u001b[33m0edda6c\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD\u001b[m\u001b[33m -> \u001b[m\u001b[1;32mmain\u001b[m\u001b[33m)\u001b[m Version 1: data splits with RANDOM_STATE=42\n",
      "\u001b[33m6661081\u001b[m Add train.ipynb with 3 benchmark models tracked by MLflow\n",
      "\u001b[33m564de3c\u001b[m Add Google Drive as DVC remote storage\n",
      "\u001b[33meaf8eeb\u001b[m configure google drive as dvc remote storage\n",
      "\u001b[33mbff705e\u001b[m configure google drive as dvc remote storage\n",
      "\u001b[33mc4f01f1\u001b[m Version 2: data splits with RANDOM_STATE=123\n",
      "\u001b[33md3baad4\u001b[m Version 1: data splits with RANDOM_STATE=42\n",
      "\u001b[33m4a140a4\u001b[m Initial commit: project setup with DVC initialization\n",
      "\u001b[33mc88235d\u001b[m Add train.ipynb with 3 benchmark models tracked by MLflow\n",
      "\u001b[33m016ac01\u001b[m Add Google Drive as DVC remote storage\n",
      "\u001b[33m70d0695\u001b[m\u001b[33m (\u001b[m\u001b[1;33mtag: \u001b[m\u001b[1;33mv2\u001b[m\u001b[33m)\u001b[m Version 2: data splits with RANDOM_STATE=123\n",
      "\u001b[33md9bca26\u001b[m\u001b[33m (\u001b[m\u001b[1;33mtag: \u001b[m\u001b[1;33mv1\u001b[m\u001b[33m)\u001b[m Version 1: data splits with RANDOM_STATE=42\n",
      "\u001b[33m2b3cda5\u001b[m Initial commit: project setup with DVC initialization\n"
     ]
    }
   ],
   "source": [
    "# Commit Version 1 (seed=42) to git\n",
    "!git add raw_data.csv.dvc train.csv.dvc validation.csv.dvc test.csv.dvc .gitignore prepare.ipynb\n",
    "!git commit -m \"Version 1: data splits with RANDOM_STATE=42\"\n",
    "!git tag v1\n",
    "print(\"\\nVersion 1 committed and tagged as 'v1'\")\n",
    "!git log --oneline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Split - Version 2 (seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split Summary:\n",
      "Total samples: 5572\n",
      "\n",
      "Training set:   3900 samples (70.0%)\n",
      "Validation set: 836 samples (15.0%)\n",
      "Test set:       836 samples (15.0%)\n",
      "\n",
      "Class distribution in each split:\n",
      "Training    : Ham=3377 (86.59%), Spam=523 (13.41%)\n",
      "Validation  : Ham= 724 (86.60%), Spam=112 (13.40%)\n",
      "Test        : Ham= 724 (86.60%), Spam=112 (13.40%)\n",
      "No data leakage detected between splits\n",
      "\n",
      "Version 2 splits (seed=123):\n",
      "  Training:   3898 samples\n",
      "  Validation: 835 samples\n",
      "  Test:       836 samples\n",
      "Files saved successfully:\n",
      "Training set:   /Users/sampriti/Downloads/cmi/AML_2/train.csv\n",
      "Validation set: /Users/sampriti/Downloads/cmi/AML_2/validation.csv\n",
      "Test set:       /Users/sampriti/Downloads/cmi/AML_2/test.csv\n"
     ]
    }
   ],
   "source": [
    "# Re-split with a different random seed\n",
    "RANDOM_STATE_V2 = 123\n",
    "np.random.seed(RANDOM_STATE_V2)\n",
    "\n",
    "# Re-split using the preprocessed dataframe (df already has cleaned_message)\n",
    "train_df_v2, val_df_v2, test_df_v2 = split_data(\n",
    "    df,\n",
    "    train_size=0.70,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    random_state=RANDOM_STATE_V2\n",
    ")\n",
    "\n",
    "# Filter empty messages\n",
    "train_df_v2 = train_df_v2[train_df_v2['cleaned_message'].str.len() > 0].copy()\n",
    "val_df_v2 = val_df_v2[val_df_v2['cleaned_message'].str.len() > 0].copy()\n",
    "test_df_v2 = test_df_v2[test_df_v2['cleaned_message'].str.len() > 0].copy()\n",
    "\n",
    "print(f\"\\nVersion 2 splits (seed=123):\")\n",
    "print(f\"  Training:   {len(train_df_v2)} samples\")\n",
    "print(f\"  Validation: {len(val_df_v2)} samples\")\n",
    "print(f\"  Test:       {len(test_df_v2)} samples\")\n",
    "\n",
    "# Save the updated splits (overwrite existing files)\n",
    "save_splits(train_df_v2, val_df_v2, test_df_v2, output_dir=PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[?25l\u001b[32m⠋\u001b[0m Checking graph\n",
      "  0% Adding...|                       | raw_data.csv |0/4 [00:00<?,     ?file/s]\n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in raw_data.csv |0.00 [00:00,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/Users/sampriti/Downloads/cmi/AML_2/.dvc/cache/files/md5\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /Users/sampriti/Downloads0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "  0% Adding...|                          | train.csv |0/4 [00:00<?,     ?file/s]\u001b[A\n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in train.csv    |0.00 [00:00,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/Users/sampriti/Downloads/cmi/AML_2/.dvc/cache/files/md5\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /Users/sampriti/Downloads0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "  0% Adding...|                     | validation.csv |0/4 [00:00<?,     ?file/s]\u001b[A\n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in validation.csv |0.00 [00:00,     ?file/\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/Users/sampriti/Downloads/cmi/AML_2/.dvc/cache/files/md5\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /Users/sampriti/Downloads0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "  0% Adding...|                           | test.csv |0/4 [00:00<?,     ?file/s]\u001b[A\n",
      "!\u001b[A\n",
      "Collecting files and computing hashes in test.csv     |0.00 [00:00,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/Users/sampriti/Downloads/cmi/AML_2/.dvc/cache/files/md5\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Checking out /Users/sampriti/Downloads0/1 [00:00<?,    ?files/s]\u001b[A\n",
      "100% Adding...|███████████████████████████████████████|4/4 [00:00, 200.26file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add test.csv.dvc train.csv.dvc raw_data.csv.dvc validation.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\u001b[0m[main f97c3d1] Version 2: data splits with RANDOM_STATE=123\n",
      " 3 files changed, 6 insertions(+), 6 deletions(-)\n",
      "fatal: tag 'v2' already exists\n",
      "\n",
      "Version 2 committed and tagged as 'v2'\n",
      "\u001b[33mf97c3d1\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD\u001b[m\u001b[33m -> \u001b[m\u001b[1;32mmain\u001b[m\u001b[33m)\u001b[m Version 2: data splits with RANDOM_STATE=123\n",
      "\u001b[33m0edda6c\u001b[m Version 1: data splits with RANDOM_STATE=42\n",
      "\u001b[33m6661081\u001b[m Add train.ipynb with 3 benchmark models tracked by MLflow\n",
      "\u001b[33m564de3c\u001b[m Add Google Drive as DVC remote storage\n",
      "\u001b[33meaf8eeb\u001b[m configure google drive as dvc remote storage\n",
      "\u001b[33mbff705e\u001b[m configure google drive as dvc remote storage\n",
      "\u001b[33mc4f01f1\u001b[m Version 2: data splits with RANDOM_STATE=123\n",
      "\u001b[33md3baad4\u001b[m Version 1: data splits with RANDOM_STATE=42\n",
      "\u001b[33m4a140a4\u001b[m Initial commit: project setup with DVC initialization\n",
      "\u001b[33mc88235d\u001b[m Add train.ipynb with 3 benchmark models tracked by MLflow\n",
      "\u001b[33m016ac01\u001b[m Add Google Drive as DVC remote storage\n",
      "\u001b[33m70d0695\u001b[m\u001b[33m (\u001b[m\u001b[1;33mtag: \u001b[m\u001b[1;33mv2\u001b[m\u001b[33m)\u001b[m Version 2: data splits with RANDOM_STATE=123\n",
      "\u001b[33md9bca26\u001b[m\u001b[33m (\u001b[m\u001b[1;33mtag: \u001b[m\u001b[1;33mv1\u001b[m\u001b[33m)\u001b[m Version 1: data splits with RANDOM_STATE=42\n",
      "\u001b[33m2b3cda5\u001b[m Initial commit: project setup with DVC initialization\n"
     ]
    }
   ],
   "source": [
    "# Track updated data with DVC\n",
    "os.chdir(PROJECT_DIR)\n",
    "!dvc add raw_data.csv train.csv validation.csv test.csv\n",
    "\n",
    "# Commit Version 2\n",
    "!git add raw_data.csv.dvc train.csv.dvc validation.csv.dvc test.csv.dvc prepare.ipynb\n",
    "!git commit -m \"Version 2: data splits with RANDOM_STATE=123\"\n",
    "!git tag v2\n",
    "print(\"\\nVersion 2 committed and tagged as 'v2'\")\n",
    "!git log --oneline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkout Versions and Compare Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label_distribution(version_name):\n",
    "    \"\"\"Load current CSV files and print ham/spam distribution.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Target Variable Distribution -- {version_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for split_name, filename in [('Train', 'train.csv'), ('Validation', 'validation.csv'), ('Test', 'test.csv')]:\n",
    "        filepath = os.path.join(PROJECT_DIR, filename)\n",
    "        split_df = pd.read_csv(filepath)\n",
    "        counts = split_df['label'].value_counts()\n",
    "        total = len(split_df)\n",
    "        print(f\"\\n{split_name} ({filename}):\")\n",
    "        print(f\"  ham:   {counts.get('ham', 0):5d}  ({counts.get('ham', 0)/total*100:.2f}%)\")\n",
    "        print(f\"  spam:  {counts.get('spam', 0):5d}  ({counts.get('spam', 0)/total*100:.2f}%)\")\n",
    "        print(f\"  total: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building workspace index                              |4.00 [00:00,  887entry/s]\n",
      "Comparing indexes                                    |5.00 [00:00, 13.3kentry/s]\n",
      "Applying changes                                      |3.00 [00:00, 4.26kfile/s]\n",
      "\u001b[33mM\u001b[0m       test.csv\n",
      "\u001b[33mM\u001b[0m       train.csv\n",
      "\u001b[33mM\u001b[0m       validation.csv\n",
      "\u001b[0m\n",
      "============================================================\n",
      "Target Variable Distribution -- Version 1 (seed=42)\n",
      "============================================================\n",
      "\n",
      "Train (train.csv):\n",
      "  ham:    3375  (86.58%)\n",
      "  spam:    523  (13.42%)\n",
      "  total: 3898\n",
      "\n",
      "Validation (validation.csv):\n",
      "  ham:     724  (86.60%)\n",
      "  spam:    112  (13.40%)\n",
      "  total: 836\n",
      "\n",
      "Test (test.csv):\n",
      "  ham:     723  (86.59%)\n",
      "  spam:    112  (13.41%)\n",
      "  total: 835\n"
     ]
    }
   ],
   "source": [
    "# Checkout Version 1 (seed=42) and print distribution\n",
    "os.chdir(PROJECT_DIR)\n",
    "!git checkout v1 -- raw_data.csv.dvc train.csv.dvc validation.csv.dvc test.csv.dvc\n",
    "!dvc checkout\n",
    "\n",
    "print_label_distribution(\"Version 1 (seed=42)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building workspace index                              |4.00 [00:00,  863entry/s]\n",
      "Comparing indexes                                    |5.00 [00:00, 13.6kentry/s]\n",
      "Applying changes                                      |3.00 [00:00, 4.49kfile/s]\n",
      "\u001b[33mM\u001b[0m       test.csv\n",
      "\u001b[33mM\u001b[0m       train.csv\n",
      "\u001b[33mM\u001b[0m       validation.csv\n",
      "\u001b[0m\n",
      "============================================================\n",
      "Target Variable Distribution -- Version 2 (seed=123)\n",
      "============================================================\n",
      "\n",
      "Train (train.csv):\n",
      "  ham:    3375  (86.58%)\n",
      "  spam:    523  (13.42%)\n",
      "  total: 3898\n",
      "\n",
      "Validation (validation.csv):\n",
      "  ham:     723  (86.59%)\n",
      "  spam:    112  (13.41%)\n",
      "  total: 835\n",
      "\n",
      "Test (test.csv):\n",
      "  ham:     724  (86.60%)\n",
      "  spam:    112  (13.40%)\n",
      "  total: 836\n"
     ]
    }
   ],
   "source": [
    "# Checkout Version 2 (seed=123) and print distribution\n",
    "os.chdir(PROJECT_DIR)\n",
    "!git checkout v2 -- raw_data.csv.dvc train.csv.dvc validation.csv.dvc test.csv.dvc\n",
    "!dvc checkout\n",
    "\n",
    "print_label_distribution(\"Version 2 (seed=123)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Configure Google Drive as DVC Remote Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dvc-gdrive -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 'gdrive' as a default remote.\n",
      "\u001b[0m\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "GDRIVE_FOLDER_ID = \"1G9H6RgIKeXkNPi-7nYFup5DYxUK3i6bD\"\n",
    "\n",
    "!dvc remote add -d gdrive gdrive://{GDRIVE_FOLDER_ID} -f\n",
    "\n",
    "!dvc remote modify gdrive gdrive_client_id ''\n",
    "!dvc remote modify gdrive gdrive_client_secret ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting                                           |4.00 [00:00, 1.47kentry/s]\n",
      "Pushing\n",
      "!\u001b[A\n",
      "  0% Checking cache in '1G9H6RgIKeXkNPi-7nYFup5DYxUK3i6bD/files/md5'| |0/? [00:0\u001b[A\n",
      "  0% Querying cache in '1G9H6RgIKeXkNPi-7nYFup5DYxUK3i6bD/files/md5'| |1/256 [00\u001b[A\n",
      "Pushing                                                                         \u001b[A\n",
      "Everything is up to date.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mkeys\u001b[m\n",
      "\t\u001b[31mmlruns/\u001b[m\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n"
     ]
    }
   ],
   "source": [
    "!git add .dvc/config\n",
    "!git commit -m \"configure google drive as dvc remote storage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mkeys\u001b[m\n",
      "\t\u001b[31mmlruns/\u001b[m\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n",
      "\u001b[33mf97c3d1\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD\u001b[m\u001b[33m -> \u001b[m\u001b[1;32mmain\u001b[m\u001b[33m)\u001b[m Version 2: data splits with RANDOM_STATE=123\n",
      "\u001b[33m0edda6c\u001b[m Version 1: data splits with RANDOM_STATE=42\n",
      "\u001b[33m6661081\u001b[m Add train.ipynb with 3 benchmark models tracked by MLflow\n",
      "\u001b[33m564de3c\u001b[m Add Google Drive as DVC remote storage\n",
      "\u001b[33meaf8eeb\u001b[m configure google drive as dvc remote storage\n",
      "\u001b[33mbff705e\u001b[m configure google drive as dvc remote storage\n",
      "\u001b[33mc4f01f1\u001b[m Version 2: data splits with RANDOM_STATE=123\n",
      "\u001b[33md3baad4\u001b[m Version 1: data splits with RANDOM_STATE=42\n",
      "\u001b[33m4a140a4\u001b[m Initial commit: project setup with DVC initialization\n",
      "\u001b[33mc88235d\u001b[m Add train.ipynb with 3 benchmark models tracked by MLflow\n",
      "\u001b[33m016ac01\u001b[m Add Google Drive as DVC remote storage\n",
      "\u001b[33m70d0695\u001b[m\u001b[33m (\u001b[m\u001b[1;33mtag: \u001b[m\u001b[1;33mv2\u001b[m\u001b[33m)\u001b[m Version 2: data splits with RANDOM_STATE=123\n",
      "\u001b[33md9bca26\u001b[m\u001b[33m (\u001b[m\u001b[1;33mtag: \u001b[m\u001b[1;33mv1\u001b[m\u001b[33m)\u001b[m Version 1: data splits with RANDOM_STATE=42\n",
      "\u001b[33m2b3cda5\u001b[m Initial commit: project setup with DVC initialization\n"
     ]
    }
   ],
   "source": [
    "# Commit remote configuration\n",
    "!git add .dvc/config\n",
    "!git commit -a -m \"Add Google Drive as DVC remote storage\"\n",
    "!git log --oneline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
