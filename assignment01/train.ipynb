{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitted by: Sampriti Mahapatra, MDS202433"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Spam Classification\n",
    "\n",
    "Comparing 3 basic classifiers using 3 metrics (precision prioritized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3900, Validation: 836, Test: 836\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('validation.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 6696\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "train_df['cleaned_message'] = train_df['cleaned_message'].fillna('')\n",
    "val_df['cleaned_message'] = val_df['cleaned_message'].fillna('')\n",
    "test_df['cleaned_message'] = test_df['cleaned_message'].fillna('')\n",
    "\n",
    "# Simple bag of words\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_df['cleaned_message'])\n",
    "X_val = vectorizer.transform(val_df['cleaned_message'])\n",
    "X_test = vectorizer.transform(test_df['cleaned_message'])\n",
    "\n",
    "y_train = (train_df['label'] == 'spam').astype(int)\n",
    "y_val = (val_df['label'] == 'spam').astype(int)\n",
    "y_test = (test_df['label'] == 'spam').astype(int)\n",
    "\n",
    "print(f\"Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Models (Train vs Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "  Metric            Train   Validation\n",
      "  Precision        1.0000       0.9720\n",
      "  Accuracy         0.9974       0.9868\n",
      "  F1-Score         0.9903       0.9498\n",
      "  False Pos             0            3\n",
      "\n",
      "Naive Bayes:\n",
      "  Metric            Train   Validation\n",
      "  Precision        0.9713       0.9537\n",
      "  Accuracy         0.9921       0.9833\n",
      "  F1-Score         0.9703       0.9364\n",
      "  False Pos            15            5\n",
      "\n",
      "Decision Tree:\n",
      "  Metric            Train   Validation\n",
      "  Precision        1.0000       0.9018\n",
      "  Accuracy         1.0000       0.9737\n",
      "  F1-Score         1.0000       0.9018\n",
      "  False Pos             0           11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"Calculate metrics and return as dict.\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return {\n",
    "        'Precision': precision_score(y_true, y_pred),\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'F1-Score': f1_score(y_true, y_pred),\n",
    "        'False Positives': fp\n",
    "    }\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Fit on train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Score on train and validation\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Evaluate on train and validation\n",
    "    train_metrics = evaluate(y_train, y_train_pred)\n",
    "    val_metrics = evaluate(y_val, y_val_pred)\n",
    "    \n",
    "    validation_results.append({\n",
    "        'Model': name,\n",
    "        'Train_Precision': train_metrics['Precision'],\n",
    "        'Val_Precision': val_metrics['Precision'],\n",
    "        'Train_Accuracy': train_metrics['Accuracy'],\n",
    "        'Val_Accuracy': val_metrics['Accuracy'],\n",
    "        'Train_F1': train_metrics['F1-Score'],\n",
    "        'Val_F1': val_metrics['F1-Score'],\n",
    "        'Train_FP': train_metrics['False Positives'],\n",
    "        'Val_FP': val_metrics['False Positives']\n",
    "    })\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  {'Metric':<12} {'Train':>10} {'Validation':>12}\")\n",
    "    print(f\"  {'Precision':<12} {train_metrics['Precision']:>10.4f} {val_metrics['Precision']:>12.4f}\")\n",
    "    print(f\"  {'Accuracy':<12} {train_metrics['Accuracy']:>10.4f} {val_metrics['Accuracy']:>12.4f}\")\n",
    "    print(f\"  {'F1-Score':<12} {train_metrics['F1-Score']:>10.4f} {val_metrics['F1-Score']:>12.4f}\")\n",
    "    print(f\"  {'False Pos':<12} {train_metrics['False Positives']:>10} {val_metrics['False Positives']:>12}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Model  Train_Precision  Val_Precision  Train_Accuracy  Val_Accuracy  Train_F1   Val_F1  Train_FP  Val_FP\n",
      "Logistic Regression         1.000000       0.971963        0.997436      0.986842  0.990347 0.949772         0       3\n",
      "        Naive Bayes         0.971264       0.953704        0.992051      0.983254  0.970335 0.936364        15       5\n",
      "      Decision Tree         1.000000       0.901786        1.000000      0.973684  1.000000 0.901786         0      11\n"
     ]
    }
   ],
   "source": [
    "val_results_df = pd.DataFrame(validation_results)\n",
    "print(val_results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "  Precision: 0.9789\n",
      "  Accuracy:  0.9749\n",
      "  F1-Score:  0.8986\n",
      "  False Positives: 2\n",
      "\n",
      "Naive Bayes:\n",
      "  Precision: 0.9612\n",
      "  Accuracy:  0.9797\n",
      "  F1-Score:  0.9209\n",
      "  False Positives: 4\n",
      "\n",
      "Decision Tree:\n",
      "  Precision: 0.8349\n",
      "  Accuracy:  0.9533\n",
      "  F1-Score:  0.8235\n",
      "  False Positives: 18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Predict on test set (models already fitted)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate(y_test, y_pred)\n",
    "    \n",
    "    test_results.append({\n",
    "        'Model': name,\n",
    "        'Precision': metrics['Precision'],\n",
    "        'Accuracy': metrics['Accuracy'],\n",
    "        'F1-Score': metrics['F1-Score'],\n",
    "        'False Positives': metrics['False Positives']\n",
    "    })\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Precision: {metrics['Precision']:.4f}\")\n",
    "    print(f\"  Accuracy:  {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score:  {metrics['F1-Score']:.4f}\")\n",
    "    print(f\"  False Positives: {metrics['False Positives']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Model  Precision  Accuracy  F1-Score  False Positives\n",
      "Logistic Regression   0.978947  0.974880  0.898551                2\n",
      "        Naive Bayes   0.961165  0.979665  0.920930                4\n",
      "      Decision Tree   0.834862  0.953349  0.823529               18\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(test_results)\n",
    "results_df = results_df.sort_values('Precision', ascending=False)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prioritise increasing precision since we want to reduce the number of false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model by Precision: Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "best_model = results_df.iloc[0]['Model']\n",
    "print(f\"Best model by Precision: {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression gives the maximum precision and least count of false positives. Maybe this is due to the simplistic nature of the dataset, it is probably linearly separable, leading to the simplistic logistic regression giving the best results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
